{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "682e41de-fad8-4dc0-b607-1b40d82f85ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniforge3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sae_lens import SAE, HookedSAETransformer\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "import ezkl\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import time\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import numpy as np\n",
    "\n",
    "#from sae_lens.toolkit.neuronpedia_integration import get_feature_from_neuronpedia\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9baeba17-2185-46bf-bd37-92b56c650eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FreivaldsVerificationSAE(SAE):\n",
    "    def __init__(self, cfg, features_to_check=None, num_trials=10, tol=0.01):\n",
    "        super().__init__(cfg)\n",
    "        self.features_to_check = [2592, 4445, 4663, 4733, 6531, 8179, 9566, 20927, 24185]   # List of feature indices to check\n",
    "        self.num_trials = num_trials\n",
    "        self.tol = tol\n",
    "        self.feature_present = False\n",
    "        \n",
    "\n",
    "    def encode_standard(self, x):\n",
    "        # Use the original encode function\n",
    "        feature_acts = super().encode_standard(x)\n",
    "\n",
    "        print(feature_acts.shape)\n",
    "\n",
    "        # Store the feature activations for later verification\n",
    "        self._last_feature_acts = feature_acts\n",
    "\n",
    "        for feature_idx in self.features_to_check:\n",
    "            # Create a copy of feature activations with the target feature zeroed out\n",
    "            modified_acts = feature_acts.clone()\n",
    "            modified_acts[:, :, feature_idx] = 0\n",
    "    \n",
    "        \n",
    "        return feature_acts\n",
    "    \n",
    "    def decode(self, feature_acts):\n",
    "        # Use the original decode function to get the reconstruction\n",
    "        sae_out = super().decode(feature_acts)\n",
    "\n",
    "        sae_out_preact = self.apply_finetuning_scaling_factor(feature_acts) @ self.W_dec + self.b_dec\n",
    "        # print(\"preact shape\", sae_out_preact.shape)\n",
    "        \n",
    "        # Verify specific features using Freivalds' algorithm\n",
    "        # if self.features_to_check is not None:\n",
    "        \n",
    "        self.verify_features(feature_acts, sae_out_preact)\n",
    "        \n",
    "            \n",
    "        return sae_out\n",
    "    \n",
    "    def verify_features(self, feature_acts, sae_out_preact):\n",
    "        \"\"\"Verifies if specific features contribute to the reconstruction using Freivalds' algorithm.\"\"\"\n",
    "        batch_size, seq_len, d_sae = feature_acts.shape\n",
    "        d_in = self.W_dec.shape[-1]\n",
    "        # print(\"Preact shape\", sae_out_preact.shape)\n",
    "        \n",
    "        \n",
    "        for feature_idx in self.features_to_check:\n",
    "            # Create a copy of feature activations with the target feature zeroed out\n",
    "            modified_acts = feature_acts.clone()\n",
    "            modified_acts[:, :, feature_idx] = 0\n",
    "            \n",
    "            # Check if reconstruction changes significantly using Freivalds\n",
    "            self.feature_present += self.freivalds_verify_reconstruction(\n",
    "                feature_acts, modified_acts, sae_out_preact\n",
    "            )\n",
    "        \n",
    "\n",
    "    def freivalds_verify_reconstruction(self, original_acts, modified_acts, sae_out_preact, num_trials=None, tol=None):\n",
    "        \"\"\"\n",
    "        Freivalds' verification integrated into SAE class\n",
    "        \"\"\"\n",
    "        d_in = self.W_dec.shape[-1]\n",
    "        self.pre_queried_random_vectors = [\n",
    "            torch.randint(0, 2, (d_in, 1), \n",
    "                      dtype=modified_acts.dtype, \n",
    "                      device=modified_acts.device)\n",
    "            for _ in range(self.num_trials)\n",
    "        ]\n",
    "\n",
    "        # batch_size, seq_len, d_sae = feature_acts.shape\n",
    "        # print(\"Shape SAE out:\", sae_out_preact.shape)\n",
    "        # print(\"Shape Preact SAE out:\", sae_out_preact.shape)\n",
    "        # print(\"Shape Decoder Bias:\", self.b_dec.shape)\n",
    "        # print(\"Shape Feature activations:\", self.apply_finetuning_scaling_factor(feature_acts).shape)\n",
    "        # print(\"Shape Decoder Weights:\", self.W_dec.shape)\n",
    "        # print(f'Seeking:{sae_out_preact - self.b_dec - self.apply_finetuning_scaling_factor(feature_acts) @ self.W_dec}')\n",
    "\n",
    "        ## Non parallelized\n",
    "        start_time = time.perf_counter()\n",
    "        \n",
    "        for i in range(self.num_trials):\n",
    "            r = self.pre_queried_random_vectors[i]\n",
    "            \n",
    "            # Compute both sides of equation\n",
    "            Cr = (sae_out_preact - self.b_dec) @ r\n",
    "            Br = self.W_dec @ r\n",
    "            ABr = self.apply_finetuning_scaling_factor(modified_acts) @ Br\n",
    "            # print(\"Original\", torch.norm((self.apply_finetuning_scaling_factor(original_acts) @ Br) - Cr))\n",
    "            # print(\"Modified\", torch.norm(ABr - Cr))\n",
    "            \n",
    "            if torch.norm(ABr - Cr, p=float('inf')) > self.tol:\n",
    "                end_time = time.perf_counter()\n",
    "                freivalds_time = end_time - start_time\n",
    "                print(f\"Freivalds verification True: {freivalds_time:.6f} seconds\")\n",
    "                return True\n",
    "            else:\n",
    "                end_time = time.perf_counter()\n",
    "                freivalds_time = end_time - start_time\n",
    "                print(f\"Freivalds verification False: {freivalds_time:.6f} seconds\")\n",
    "        return False\n",
    "\n",
    "\n",
    "\n",
    "        ## Parallelized ## Will Require HF Parallelism diabling\n",
    "        # def verify_trial(args):\n",
    "        #     i, vectors_list, W_dec_np, b_dec_np, scaled_acts_np, sae_out_preact_np, tol = args\n",
    "            \n",
    "        #     # Get the pre-generated random vector for this trial\n",
    "        #     r_np = vectors_list[i]\n",
    "            \n",
    "        #     # Compute both sides of equation with NumPy\n",
    "        #     Cr_np = (sae_out_preact_np - b_dec_np) @ r_np\n",
    "        #     Br_np = W_dec_np @ r_np\n",
    "        #     ABr_np = scaled_acts_np @ Br_np\n",
    "            \n",
    "        #     # Calculate norm using NumPy\n",
    "        #     norm_val = np.max(np.abs(ABr_np - Cr_np))\n",
    "        #     return norm_val > tol\n",
    "        \n",
    "        # \"\"\"\n",
    "        # Parallelized Freivalds' verification using multiprocessing\n",
    "        # \"\"\"\n",
    "        # # Apply scaling factor before converting to NumPy\n",
    "        # scaled_acts = self.apply_finetuning_scaling_factor(modified_acts)\n",
    "        \n",
    "        # # Convert PyTorch tensors to NumPy arrays for multiprocessing\n",
    "        # scaled_acts_np = scaled_acts.cpu().detach().numpy()\n",
    "        # sae_out_preact_np = sae_out_preact.cpu().detach().numpy()\n",
    "        # W_dec_np = self.W_dec.cpu().detach().numpy()\n",
    "        # b_dec_np = self.b_dec.cpu().detach().numpy()\n",
    "        # vectors_list = [r.cpu().detach().numpy() for r in self.pre_queried_random_vectors]\n",
    "        \n",
    "        # start_time = time.perf_counter()\n",
    "        \n",
    "        # args = [(i, vectors_list, W_dec_np, b_dec_np, scaled_acts_np, sae_out_preact_np, self.tol) \n",
    "        #         for i in range(self.num_trials)]\n",
    "        \n",
    "        # with Pool(processes=cpu_count()) as pool:\n",
    "        #     results = pool.map(verify_trial, args)\n",
    "            \n",
    "        # end_time = time.perf_counter()\n",
    "        # freivalds_time = end_time - start_time\n",
    "        \n",
    "        # if any(results):\n",
    "        #     print(f\"Freivalds verification True: {freivalds_time:.6f} seconds\")\n",
    "        #     return True\n",
    "        # else:\n",
    "        #     print(f\"Freivalds verification False: {freivalds_time:.6f} seconds\")\n",
    "        #     return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67d0243b-4264-4ab2-ab6e-36d0edab9230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniforge3/lib/python3.12/site-packages/sae_lens/sae.py:151: UserWarning: \n",
      "This SAE has non-empty model_from_pretrained_kwargs. \n",
      "For optimal performance, load the model like so:\n",
      "model = HookedSAETransformer.from_pretrained_no_processing(..., **cfg.model_from_pretrained_kwargs)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = HookedSAETransformer.from_pretrained(\"gpt2-small\", device=\"cpu\")\n",
    "\n",
    "# the cfg dict is returned alongside the SAE since it may contain useful information for analysing the SAE (eg: instantiating an activation store)\n",
    "# Note that this is not the same as the SAEs config dict, rather it is whatever was in the HF repo, from which we can extract the SAE config dict\n",
    "# We also return the feature sparsities which are stored in HF for convenience.\n",
    "sae, cfg_dict, sparsity = FreivaldsVerificationSAE.from_pretrained(\n",
    "    release=\"gpt2-small-res-jb\",  # <- Release name\n",
    "    sae_id=\"blocks.7.hook_resid_pre\",  # <- SAE id (not always a hook point!)\n",
    "    device=\"cpu\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5c318ac6-92bb-4237-b334-a682dca1a0f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'architecture': 'standard', 'd_in': 768, 'd_sae': 24576, 'activation_fn_str': 'relu', 'apply_b_dec_to_input': True, 'finetuning_scaling_factor': False, 'context_size': 128, 'model_name': 'gpt2-small', 'hook_name': 'blocks.7.hook_resid_pre', 'hook_layer': 7, 'hook_head_index': None, 'prepend_bos': True, 'dataset_path': 'Skylion007/openwebtext', 'dataset_trust_remote_code': True, 'normalize_activations': 'none', 'dtype': 'torch.float32', 'device': 'cpu', 'sae_lens_training_version': None, 'activation_fn_kwargs': {}, 'neuronpedia_id': 'gpt2-small/7-res-jb', 'model_from_pretrained_kwargs': {'center_writing_weights': True}, 'seqpos_slice': (None,)}\n"
     ]
    }
   ],
   "source": [
    "print(sae.cfg.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad089df4-04e8-44ca-b44c-58e2166b9371",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HookedSAETransformer(\n",
      "  (embed): Embed()\n",
      "  (hook_embed): HookPoint()\n",
      "  (pos_embed): PosEmbed()\n",
      "  (hook_pos_embed): HookPoint()\n",
      "  (blocks): ModuleList(\n",
      "    (0-11): 12 x TransformerBlock(\n",
      "      (ln1): LayerNormPre(\n",
      "        (hook_scale): HookPoint()\n",
      "        (hook_normalized): HookPoint()\n",
      "      )\n",
      "      (ln2): LayerNormPre(\n",
      "        (hook_scale): HookPoint()\n",
      "        (hook_normalized): HookPoint()\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        (hook_k): HookPoint()\n",
      "        (hook_q): HookPoint()\n",
      "        (hook_v): HookPoint()\n",
      "        (hook_z): HookPoint()\n",
      "        (hook_attn_scores): HookPoint()\n",
      "        (hook_pattern): HookPoint()\n",
      "        (hook_result): HookPoint()\n",
      "      )\n",
      "      (mlp): MLP(\n",
      "        (hook_pre): HookPoint()\n",
      "        (hook_post): HookPoint()\n",
      "      )\n",
      "      (hook_attn_in): HookPoint()\n",
      "      (hook_q_input): HookPoint()\n",
      "      (hook_k_input): HookPoint()\n",
      "      (hook_v_input): HookPoint()\n",
      "      (hook_mlp_in): HookPoint()\n",
      "      (hook_attn_out): HookPoint()\n",
      "      (hook_mlp_out): HookPoint()\n",
      "      (hook_resid_pre): HookPoint()\n",
      "      (hook_resid_mid): HookPoint()\n",
      "      (hook_resid_post): HookPoint()\n",
      "    )\n",
      "  )\n",
      "  (ln_final): LayerNormPre(\n",
      "    (hook_scale): HookPoint()\n",
      "    (hook_normalized): HookPoint()\n",
      "  )\n",
      "  (unembed): Unembed()\n",
      ")\n",
      "HookedTransformerConfig:\n",
      "{'NTK_by_parts_factor': 8.0,\n",
      " 'NTK_by_parts_high_freq_factor': 4.0,\n",
      " 'NTK_by_parts_low_freq_factor': 1.0,\n",
      " 'act_fn': 'gelu_new',\n",
      " 'attention_dir': 'causal',\n",
      " 'attn_only': False,\n",
      " 'attn_scale': 8.0,\n",
      " 'attn_scores_soft_cap': -1.0,\n",
      " 'attn_types': None,\n",
      " 'checkpoint_index': None,\n",
      " 'checkpoint_label_type': None,\n",
      " 'checkpoint_value': None,\n",
      " 'd_head': 64,\n",
      " 'd_mlp': 3072,\n",
      " 'd_model': 768,\n",
      " 'd_vocab': 50257,\n",
      " 'd_vocab_out': 50257,\n",
      " 'decoder_start_token_id': None,\n",
      " 'default_prepend_bos': True,\n",
      " 'device': 'cpu',\n",
      " 'dtype': torch.float32,\n",
      " 'eps': 1e-05,\n",
      " 'experts_per_token': None,\n",
      " 'final_rms': False,\n",
      " 'from_checkpoint': False,\n",
      " 'gated_mlp': False,\n",
      " 'init_mode': 'gpt2',\n",
      " 'init_weights': False,\n",
      " 'initializer_range': 0.02886751345948129,\n",
      " 'load_in_4bit': False,\n",
      " 'model_name': 'gpt2',\n",
      " 'n_ctx': 1024,\n",
      " 'n_devices': 1,\n",
      " 'n_heads': 12,\n",
      " 'n_key_value_heads': None,\n",
      " 'n_layers': 12,\n",
      " 'n_params': 84934656,\n",
      " 'normalization_type': 'LNPre',\n",
      " 'num_experts': None,\n",
      " 'original_architecture': 'GPT2LMHeadModel',\n",
      " 'output_logits_soft_cap': -1.0,\n",
      " 'parallel_attn_mlp': False,\n",
      " 'positional_embedding_type': 'standard',\n",
      " 'post_embedding_ln': False,\n",
      " 'relative_attention_max_distance': None,\n",
      " 'relative_attention_num_buckets': None,\n",
      " 'rotary_adjacent_pairs': False,\n",
      " 'rotary_base': 10000,\n",
      " 'rotary_dim': None,\n",
      " 'scale_attn_by_inverse_layer_idx': False,\n",
      " 'seed': None,\n",
      " 'tie_word_embeddings': False,\n",
      " 'tokenizer_name': 'gpt2',\n",
      " 'tokenizer_prepends_bos': False,\n",
      " 'trust_remote_code': False,\n",
      " 'ungroup_grouped_query_attention': False,\n",
      " 'use_NTK_by_parts_rope': False,\n",
      " 'use_attn_in': False,\n",
      " 'use_attn_result': False,\n",
      " 'use_attn_scale': True,\n",
      " 'use_hook_mlp_in': False,\n",
      " 'use_hook_tokens': False,\n",
      " 'use_local_attn': False,\n",
      " 'use_normalization_before_and_after': False,\n",
      " 'use_split_qkv_input': False,\n",
      " 'window_size': None}\n"
     ]
    }
   ],
   "source": [
    "print(model)  # Or:\n",
    "print(model.cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be14697c-cef4-4dbe-bc44-43189072cb5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 373/373 [00:00<00:00, 18.6kB/s]\n",
      "Downloading metadata: 100%|██████████| 921/921 [00:00<00:00, 48.8kB/s]\n",
      "Downloading data: 100%|██████████| 33.3M/33.3M [00:00<00:00, 290MB/s]\n",
      "Generating train split: 100%|██████████| 10000/10000 [00:01<00:00, 9181.56 examples/s]\n",
      "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (229134 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Map: 100%|██████████| 10000/10000 [00:20<00:00, 490.98 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformer_lens.utils import tokenize_and_concatenate\n",
    "\n",
    "dataset = load_dataset(\n",
    "    path=\"NeelNanda/pile-10k\",\n",
    "    split=\"train\",\n",
    "    streaming=False,\n",
    ")\n",
    "\n",
    "token_dataset = tokenize_and_concatenate(\n",
    "    dataset=dataset,  # type: ignore\n",
    "    tokenizer=model.tokenizer,  # type: ignore\n",
    "    streaming=True,\n",
    "    max_length=sae.cfg.context_size,\n",
    "    add_bos_token=sae.cfg.prepend_bos,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e05ed46-2c21-4bf8-8c3b-727149bbdff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://www.neuronpedia.org/api/explanation/export?modelId=gpt2-small&saeId=7-res-jb\"\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "response = requests.get(url, headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5361b6cb-2a36-42e6-a4c5-a10ca13e4284",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>modelId</th>\n",
       "      <th>layer</th>\n",
       "      <th>feature</th>\n",
       "      <th>description</th>\n",
       "      <th>explanationModelName</th>\n",
       "      <th>typeName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt2-small</td>\n",
       "      <td>7-res-jb</td>\n",
       "      <td>218</td>\n",
       "      <td>stars and dashed for censoring expletives</td>\n",
       "      <td>None</td>\n",
       "      <td>oai_token-act-pair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt2-small</td>\n",
       "      <td>7-res-jb</td>\n",
       "      <td>218</td>\n",
       "      <td>stars and dashes for censoring expletives</td>\n",
       "      <td>None</td>\n",
       "      <td>oai_token-act-pair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpt2-small</td>\n",
       "      <td>7-res-jb</td>\n",
       "      <td>218</td>\n",
       "      <td>offensive language and expletives</td>\n",
       "      <td>None</td>\n",
       "      <td>oai_token-act-pair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gpt2-small</td>\n",
       "      <td>7-res-jb</td>\n",
       "      <td>2020</td>\n",
       "      <td>names of people</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>oai_token-act-pair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gpt2-small</td>\n",
       "      <td>7-res-jb</td>\n",
       "      <td>3493</td>\n",
       "      <td>references to nazism</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>oai_token-act-pair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24568</th>\n",
       "      <td>gpt2-small</td>\n",
       "      <td>7-res-jb</td>\n",
       "      <td>24571</td>\n",
       "      <td>locations and cities paired with information s...</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>oai_token-act-pair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24569</th>\n",
       "      <td>gpt2-small</td>\n",
       "      <td>7-res-jb</td>\n",
       "      <td>24572</td>\n",
       "      <td>actions related to personal grooming, such as ...</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>oai_token-act-pair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24570</th>\n",
       "      <td>gpt2-small</td>\n",
       "      <td>7-res-jb</td>\n",
       "      <td>24573</td>\n",
       "      <td>words containing the sequence \"lo\"</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>oai_token-act-pair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24571</th>\n",
       "      <td>gpt2-small</td>\n",
       "      <td>7-res-jb</td>\n",
       "      <td>24574</td>\n",
       "      <td>instances of added or inserted text</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>oai_token-act-pair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24572</th>\n",
       "      <td>gpt2-small</td>\n",
       "      <td>7-res-jb</td>\n",
       "      <td>24575</td>\n",
       "      <td>positive aspects or characteristics of a creat...</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>oai_token-act-pair</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24573 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          modelId     layer feature  \\\n",
       "0      gpt2-small  7-res-jb     218   \n",
       "1      gpt2-small  7-res-jb     218   \n",
       "2      gpt2-small  7-res-jb     218   \n",
       "3      gpt2-small  7-res-jb    2020   \n",
       "4      gpt2-small  7-res-jb    3493   \n",
       "...           ...       ...     ...   \n",
       "24568  gpt2-small  7-res-jb   24571   \n",
       "24569  gpt2-small  7-res-jb   24572   \n",
       "24570  gpt2-small  7-res-jb   24573   \n",
       "24571  gpt2-small  7-res-jb   24574   \n",
       "24572  gpt2-small  7-res-jb   24575   \n",
       "\n",
       "                                             description explanationModelName  \\\n",
       "0              stars and dashed for censoring expletives                 None   \n",
       "1              stars and dashes for censoring expletives                 None   \n",
       "2                      offensive language and expletives                 None   \n",
       "3                                        names of people        gpt-3.5-turbo   \n",
       "4                                   references to nazism        gpt-3.5-turbo   \n",
       "...                                                  ...                  ...   \n",
       "24568  locations and cities paired with information s...        gpt-3.5-turbo   \n",
       "24569  actions related to personal grooming, such as ...        gpt-3.5-turbo   \n",
       "24570                 words containing the sequence \"lo\"        gpt-3.5-turbo   \n",
       "24571                instances of added or inserted text        gpt-3.5-turbo   \n",
       "24572  positive aspects or characteristics of a creat...        gpt-3.5-turbo   \n",
       "\n",
       "                 typeName  \n",
       "0      oai_token-act-pair  \n",
       "1      oai_token-act-pair  \n",
       "2      oai_token-act-pair  \n",
       "3      oai_token-act-pair  \n",
       "4      oai_token-act-pair  \n",
       "...                   ...  \n",
       "24568  oai_token-act-pair  \n",
       "24569  oai_token-act-pair  \n",
       "24570  oai_token-act-pair  \n",
       "24571  oai_token-act-pair  \n",
       "24572  oai_token-act-pair  \n",
       "\n",
       "[24573 rows x 6 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert to pandas\n",
    "data = response.json()\n",
    "explanations_df = pd.DataFrame(data)\n",
    "# rename index to \"feature\"\n",
    "explanations_df.rename(columns={\"index\": \"feature\"}, inplace=True)\n",
    "# explanations_df[\"feature\"] = explanations_df[\"feature\"].astype(int)\n",
    "explanations_df[\"description\"] = explanations_df[\"description\"].apply(\n",
    "    lambda x: x.lower()\n",
    ")\n",
    "explanations_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "679cc555-f190-4f0c-b6c8-29dfb96fd4fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>modelId</th>\n",
       "      <th>layer</th>\n",
       "      <th>feature</th>\n",
       "      <th>description</th>\n",
       "      <th>explanationModelName</th>\n",
       "      <th>typeName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11902</th>\n",
       "      <td>gpt2-small</td>\n",
       "      <td>7-res-jb</td>\n",
       "      <td>11735</td>\n",
       "      <td>references to religious texts, particularly th...</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>oai_token-act-pair</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          modelId     layer feature  \\\n",
       "11902  gpt2-small  7-res-jb   11735   \n",
       "\n",
       "                                             description explanationModelName  \\\n",
       "11902  references to religious texts, particularly th...        gpt-3.5-turbo   \n",
       "\n",
       "                 typeName  \n",
       "11902  oai_token-act-pair  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bible_features = explanations_df.loc[explanations_df.description.str.contains(\" bible\")]\n",
    "bible_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba959915-26c6-48fa-9994-1888ba914f02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'Today', ' is', ' weekend', ',', ' tomorrow', ' is']\n",
      "Tokenized answer: [' Monday']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13.96</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8.18</span><span style=\"font-weight: bold\">% Token: | Monday|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m13.96\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m8.18\u001b[0m\u001b[1m% Token: | Monday|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 13.96 Prob:  8.18% Token: | Monday|\n",
      "Top 1th token. Logit: 13.85 Prob:  7.39% Token: | Saturday|\n",
      "Top 2th token. Logit: 13.67 Prob:  6.15% Token: | Sunday|\n",
      "Top 3th token. Logit: 13.63 Prob:  5.89% Token: | weekend|\n",
      "Top 4th token. Logit: 13.34 Prob:  4.43% Token: | the|\n",
      "Top 5th token. Logit: 12.85 Prob:  2.70% Token: | week|\n",
      "Top 6th token. Logit: 12.70 Prob:  2.34% Token: | day|\n",
      "Top 7th token. Logit: 12.67 Prob:  2.27% Token: | Friday|\n",
      "Top 8th token. Logit: 12.58 Prob:  2.07% Token: | holiday|\n",
      "Top 9th token. Logit: 12.56 Prob:  2.02% Token: | a|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' Monday'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' Monday'\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformer_lens.utils import test_prompt\n",
    "\n",
    "prompt = \"How do i live my life\"\n",
    "promp2 = \"Today is weekend, tomorrow is\"\n",
    "answer = \"Monday\"\n",
    "\n",
    "# Show that the model can confidently predict the next token.\n",
    "test_prompt(promp2, answer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "001f2a10-6ba3-42cf-a1f6-a782fa4b2755",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sae.use_error_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0458e33-46c0-4a1e-9fa7-cd8cc124eb0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 7, 24576])\n",
      "Freivalds verification False: 0.005411 seconds\n",
      "Freivalds verification False: 0.009720 seconds\n",
      "Freivalds verification False: 0.013746 seconds\n",
      "Freivalds verification False: 0.017730 seconds\n",
      "Freivalds verification False: 0.021717 seconds\n",
      "Freivalds verification False: 0.025689 seconds\n",
      "Freivalds verification False: 0.029657 seconds\n",
      "Freivalds verification False: 0.033612 seconds\n",
      "Freivalds verification False: 0.037578 seconds\n",
      "Freivalds verification False: 0.041543 seconds\n",
      "Freivalds verification False: 0.003775 seconds\n",
      "Freivalds verification False: 0.007568 seconds\n",
      "Freivalds verification False: 0.011322 seconds\n",
      "Freivalds verification False: 0.015075 seconds\n",
      "Freivalds verification False: 0.018836 seconds\n",
      "Freivalds verification False: 0.022569 seconds\n",
      "Freivalds verification False: 0.026321 seconds\n",
      "Freivalds verification False: 0.030066 seconds\n",
      "Freivalds verification False: 0.033811 seconds\n",
      "Freivalds verification False: 0.039085 seconds\n",
      "Freivalds verification False: 0.003740 seconds\n",
      "Freivalds verification False: 0.007528 seconds\n",
      "Freivalds verification False: 0.011293 seconds\n",
      "Freivalds verification False: 0.015052 seconds\n",
      "Freivalds verification False: 0.018810 seconds\n",
      "Freivalds verification False: 0.022571 seconds\n",
      "Freivalds verification False: 0.026327 seconds\n",
      "Freivalds verification False: 0.030097 seconds\n",
      "Freivalds verification False: 0.034065 seconds\n",
      "Freivalds verification False: 0.038018 seconds\n",
      "Freivalds verification False: 0.003904 seconds\n",
      "Freivalds verification False: 0.007862 seconds\n",
      "Freivalds verification False: 0.011806 seconds\n",
      "Freivalds verification False: 0.015745 seconds\n",
      "Freivalds verification False: 0.019682 seconds\n",
      "Freivalds verification False: 0.023630 seconds\n",
      "Freivalds verification False: 0.027565 seconds\n",
      "Freivalds verification False: 0.031507 seconds\n",
      "Freivalds verification False: 0.035430 seconds\n",
      "Freivalds verification False: 0.039378 seconds\n",
      "Freivalds verification False: 0.003940 seconds\n",
      "Freivalds verification False: 0.007948 seconds\n",
      "Freivalds verification False: 0.011936 seconds\n",
      "Freivalds verification False: 0.015912 seconds\n",
      "Freivalds verification False: 0.021404 seconds\n",
      "Freivalds verification False: 0.025583 seconds\n",
      "Freivalds verification False: 0.029432 seconds\n",
      "Freivalds verification False: 0.033228 seconds\n",
      "Freivalds verification False: 0.037001 seconds\n",
      "Freivalds verification False: 0.040768 seconds\n",
      "Freivalds verification False: 0.003713 seconds\n",
      "Freivalds verification False: 0.007502 seconds\n",
      "Freivalds verification False: 0.011285 seconds\n",
      "Freivalds verification False: 0.015069 seconds\n",
      "Freivalds verification False: 0.018883 seconds\n",
      "Freivalds verification False: 0.022665 seconds\n",
      "Freivalds verification False: 0.026445 seconds\n",
      "Freivalds verification False: 0.030241 seconds\n",
      "Freivalds verification False: 0.034010 seconds\n",
      "Freivalds verification False: 0.037786 seconds\n",
      "Freivalds verification True: 0.003730 seconds\n",
      "Freivalds verification True: 0.003723 seconds\n",
      "Freivalds verification False: 0.003737 seconds\n",
      "Freivalds verification False: 0.007720 seconds\n",
      "Freivalds verification False: 0.011725 seconds\n",
      "Freivalds verification False: 0.015696 seconds\n",
      "Freivalds verification False: 0.019694 seconds\n",
      "Freivalds verification False: 0.023692 seconds\n",
      "Freivalds verification False: 0.027866 seconds\n",
      "Freivalds verification False: 0.033346 seconds\n",
      "Freivalds verification False: 0.037569 seconds\n",
      "Freivalds verification False: 0.041570 seconds\n",
      "[('blocks.7.hook_resid_pre.hook_sae_input', torch.Size([1, 7, 768])), ('blocks.7.hook_resid_pre.hook_sae_acts_pre', torch.Size([1, 7, 24576])), ('blocks.7.hook_resid_pre.hook_sae_acts_post', torch.Size([1, 7, 24576])), ('blocks.7.hook_resid_pre.hook_sae_recons', torch.Size([1, 7, 768])), ('blocks.7.hook_resid_pre.hook_sae_output', torch.Size([1, 7, 768]))]\n"
     ]
    }
   ],
   "source": [
    "# hooked SAE Transformer will enable us to get the feature activations from the SAE\n",
    "sae.feature_present = 0\n",
    "_, cache = model.run_with_cache_with_saes(promp2, saes=[sae])\n",
    "\n",
    "print([(k, v.shape) for k, v in cache.items() if \"sae\" in k])\n",
    "\n",
    "# note there were 11 tokens in our prompt, the residual stream dimension is 768, and the number of SAE features is 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ad915440-5350-4782-bda6-7c8d9c509871",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sae.feature_present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "78e5fea0-b170-4222-8a13-d5ab893f44fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DirectVerificationSAE(SAE):\n",
    "    def __init__(self, cfg, features_to_check=None, num_trials=10, tol=0.01):\n",
    "        super().__init__(cfg)\n",
    "        self.features_to_check = [2592, 4445, 4663, 4733, 6531, 8179, 9566, 20927, 24185]   # List of feature indices to check\n",
    "        self.num_trials = num_trials\n",
    "        self.tol = tol\n",
    "        self.feature_present = False\n",
    "        \n",
    "\n",
    "    def encode_standard(self, x):\n",
    "        # Use the original encode function\n",
    "        feature_acts = super().encode_standard(x)\n",
    "\n",
    "        print(feature_acts.shape)\n",
    "\n",
    "        # Store the feature activations for later verification\n",
    "        self._last_feature_acts = feature_acts\n",
    "\n",
    "        for feature_idx in self.features_to_check:\n",
    "            # Create a copy of feature activations with the target feature zeroed out\n",
    "            modified_acts = feature_acts.clone()\n",
    "            modified_acts[:, :, feature_idx] = 0\n",
    "    \n",
    "        \n",
    "        return feature_acts\n",
    "    \n",
    "    def decode(self, feature_acts):\n",
    "        # Use the original decode function to get the reconstruction\n",
    "        sae_out = super().decode(feature_acts)\n",
    "\n",
    "        sae_out_preact = self.apply_finetuning_scaling_factor(feature_acts) @ self.W_dec + self.b_dec\n",
    "        # print(\"preact shape\", sae_out_preact.shape)\n",
    "        \n",
    "        # Verify specific features using Freivalds' algorithm\n",
    "        # if self.features_to_check is not None:\n",
    "        \n",
    "        self.verify_features(feature_acts, sae_out_preact)\n",
    "        \n",
    "            \n",
    "        return sae_out\n",
    "    \n",
    "    def verify_features(self, feature_acts, sae_out_preact):\n",
    "        \"\"\"Verifies if specific features contribute to the reconstruction using Freivalds' algorithm.\"\"\"\n",
    "        batch_size, seq_len, d_sae = feature_acts.shape\n",
    "        d_in = self.W_dec.shape[-1]\n",
    "        # print(\"Preact shape\", sae_out_preact.shape)\n",
    "        \n",
    "        \n",
    "        for feature_idx in self.features_to_check:\n",
    "            # Create a copy of feature activations with the target feature zeroed out\n",
    "            modified_acts = feature_acts.clone()\n",
    "            modified_acts[:, :, feature_idx] = 0\n",
    "            \n",
    "            # Check if reconstruction changes significantly using Freivalds\n",
    "            self.feature_present += self.regular_verify_reconstruction(\n",
    "                feature_acts, modified_acts, sae_out_preact\n",
    "            )\n",
    "        \n",
    "\n",
    "    def regular_verify_reconstruction(self, original_acts, modified_acts, sae_out_preact, num_trials=None, tol=None):\n",
    "        \"\"\"\n",
    "        Regular verification integrated into SAE class\n",
    "        \"\"\"\n",
    "        d_in = self.W_dec.shape[-1]\n",
    "\n",
    "        # batch_size, seq_len, d_sae = feature_acts.shape\n",
    "        # print(\"Shape SAE out:\", sae_out_preact.shape)\n",
    "        # print(\"Shape Preact SAE out:\", sae_out_preact.shape)\n",
    "        # print(\"Shape Decoder Bias:\", self.b_dec.shape)\n",
    "        # print(\"Shape Feature activations:\", self.apply_finetuning_scaling_factor(feature_acts).shape)\n",
    "        # print(\"Shape Decoder Weights:\", self.W_dec.shape)\n",
    "        # print(f'Seeking:{sae_out_preact - self.b_dec - self.apply_finetuning_scaling_factor(feature_acts) @ self.W_dec}')\n",
    "        start_time = time.perf_counter()\n",
    "        \n",
    "        for i in range(self.num_trials):\n",
    "            \n",
    "            # Compute both sides of equation\n",
    "            C = (sae_out_preact - self.b_dec)\n",
    "            AB = self.apply_finetuning_scaling_factor(modified_acts) @ self.W_dec\n",
    "            # print(\"Original\", torch.norm((self.apply_finetuning_scaling_factor(original_acts) @ Br) - Cr))\n",
    "            # print(\"Modified\", torch.norm(ABr - Cr))\n",
    "            \n",
    "            if torch.norm(AB - C, p=float('inf')) > self.tol:\n",
    "                end_time = time.perf_counter()\n",
    "                freivalds_time = end_time - start_time\n",
    "                print(f\"Normal verification True: {freivalds_time:.6f} seconds\")\n",
    "                return True\n",
    "            else:\n",
    "                end_time = time.perf_counter()\n",
    "                freivalds_time = end_time - start_time\n",
    "                print(f\"Normal verification False: {freivalds_time:.6f} seconds\")\n",
    "                return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9a6ecca8-6528-47ea-b7e9-371676cc972b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model = HookedSAETransformer.from_pretrained(\"gpt2-small\", device=\"cpu\")\n",
    "\n",
    "# the cfg dict is returned alongside the SAE since it may contain useful information for analysing the SAE (eg: instantiating an activation store)\n",
    "# Note that this is not the same as the SAEs config dict, rather it is whatever was in the HF repo, from which we can extract the SAE config dict\n",
    "# We also return the feature sparsities which are stored in HF for convenience.\n",
    "sae, cfg_dict, sparsity = DirectVerificationSAE.from_pretrained(\n",
    "    release=\"gpt2-small-res-jb\",  # <- Release name\n",
    "    sae_id=\"blocks.7.hook_resid_pre\",  # <- SAE id (not always a hook point!)\n",
    "    device=\"cpu\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0a606635-56d0-41d3-9707-18dc1d994ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 7, 24576])\n",
      "Normal verification False: 0.005809 seconds\n",
      "Normal verification False: 0.005704 seconds\n",
      "Normal verification False: 0.005648 seconds\n",
      "Normal verification False: 0.005600 seconds\n",
      "Normal verification False: 0.005579 seconds\n",
      "Normal verification False: 0.005529 seconds\n",
      "Normal verification True: 0.005388 seconds\n",
      "Normal verification True: 0.005364 seconds\n",
      "Normal verification False: 0.005509 seconds\n"
     ]
    }
   ],
   "source": [
    "sae.feature_present = 0\n",
    "_, cache = model.run_with_cache_with_saes(promp2, saes=[sae])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "23ce8a2c-dd18-4f2e-aa9d-deec935b2a33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sae.feature_present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8f1ad6d9-caff-40a9-9246-2773e097ee73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model = HookedSAETransformer.from_pretrained(\"gpt2-small\", device=\"cpu\")\n",
    "\n",
    "# the cfg dict is returned alongside the SAE since it may contain useful information for analysing the SAE (eg: instantiating an activation store)\n",
    "# Note that this is not the same as the SAEs config dict, rather it is whatever was in the HF repo, from which we can extract the SAE config dict\n",
    "# We also return the feature sparsities which are stored in HF for convenience.\n",
    "sae, cfg_dict, sparsity = TimedVerificationSAE.from_pretrained(\n",
    "    release=\"gpt2-small-res-jb\",  # <- Release name\n",
    "    sae_id=\"blocks.7.hook_resid_pre\",  # <- SAE id (not always a hook point!)\n",
    "    device=\"cpu\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1f6027bf-6a5c-4873-af01-dea17818588b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  9.08it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  9.17it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  9.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.35it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.71it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  9.14it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.87it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.93it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.90it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.96it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.64it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  9.55it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 10.47it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 10.25it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 10.01it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  9.58it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  9.72it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  9.76it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  9.73it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.21it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.22it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.84it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  6.34it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.71it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  7.54it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  6.98it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.10it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.10it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.15it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.14it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.12it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  7.34it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.06it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.03it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.11it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.11it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.12it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  7.37it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  9.48it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  9.50it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  9.59it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  9.57it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  9.64it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  9.30it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  9.26it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  9.30it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  9.26it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  9.30it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.79it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.24it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.27it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.29it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  9.16it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.37it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.64it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.09it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.27it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  7.68it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.88it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  7.77it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.46it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.49it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  7.84it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  7.90it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  7.91it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  7.88it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  7.06it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  6.10it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.73it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.03it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.11it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.34it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.40it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.49it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.95it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  7.78it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  7.86it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  7.92it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  7.95it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  7.92it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  7.95it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.41it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  7.54it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  7.74it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  7.82it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.44it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  7.75it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  7.74it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  7.75it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  7.90it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.82it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.36it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.33it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.34it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.67it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.12it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  7.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average generation time: 0.141705 seconds\n",
      "Standard deviation: 0.068625 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "prompt = \"Today is weekend, tomorrow is\"\n",
    "num_runs = 100\n",
    "generation_times = []\n",
    "\n",
    "for _ in range(num_runs):\n",
    "    # Start timing for generation\n",
    "    start_time = time.perf_counter()\n",
    "    \n",
    "    # Generate a response\n",
    "    input_ids = model.tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    output = model.generate(input_ids, max_new_tokens=1)\n",
    "    response = model.tokenizer.decode(output[0][-1:])\n",
    "    \n",
    "    # End timing for generation\n",
    "    end_time = time.perf_counter()\n",
    "    generation_times.append(end_time - start_time)\n",
    "\n",
    "# Calculate average and standard deviation\n",
    "average_generation_time = np.mean(generation_times)\n",
    "std_dev = np.std(generation_times)\n",
    "\n",
    "print(f\"Average generation time: {average_generation_time:.6f} seconds\")\n",
    "print(f\"Standard deviation: {std_dev:.6f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0713733f-9b7e-4eec-ac32-d394a34177f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  5.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Today is weekend, tomorrow is\n",
      "Model response:  Saturday\n",
      "Is weekday: True\n",
      "Generation time (seconds): 0.170548\n",
      "Verification time (seconds): 0.000050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Today is weekend, tomorrow is\"\n",
    "\n",
    "start_time1, end_time1 = 0, 0\n",
    "# Start timing for generation\n",
    "start_time1 = time.perf_counter()\n",
    "\n",
    "# Generate a response\n",
    "input_ids = model.tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "output = model.generate(input_ids, max_new_tokens=1)\n",
    "response = model.tokenizer.decode(output[0][-1:])\n",
    "\n",
    "# End timing for generation\n",
    "end_time1 = time.perf_counter()\n",
    "generation_time = end_time1 - start_time1\n",
    "\n",
    "# Regex pattern to match feature\n",
    "weekday_pattern = re.compile(r\"^(mon|monday|tue|tues|tuesday|wed|wednesday|thu|thurs|thursday|fri|friday|sat|saturday|sun|sunday)$\", re.IGNORECASE)\n",
    "\n",
    "# Start timing for generation\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "# Check if the response is a weekday\n",
    "is_weekday = bool(weekday_pattern.match(response.strip()))\n",
    "\n",
    "# End timing for generation\n",
    "end_time = time.perf_counter()\n",
    "verif_time = end_time - start_time\n",
    "\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Model response: {response}\")\n",
    "print(f\"Is weekday: {is_weekday}\")\n",
    "print(f\"Generation time (seconds): {generation_time:.6f}\")\n",
    "print(f\"Verification time (seconds): {verif_time:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1aa92549-0dc5-477a-9df1-49cbf3666b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Today is weekend, tomorrow is\n",
      "Model response:  holiday\n",
      "Is weekday: False\n",
      "Confidence: 0.9999\n",
      "Generation time (seconds): 0.249698\n",
      "Classification time (seconds): 1.048472\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "\n",
    "# Load RoBERTa tokenizer and model for classification\n",
    "tokenizer_roberta = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "classifier = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=2)\n",
    "classifier.eval()  # Set model to evaluation mode\n",
    "\n",
    "# Define the prompt\n",
    "prompt = \"Today is weekend, tomorrow is\"\n",
    "\n",
    "# Start timing for generation\n",
    "start_gen_time = time.perf_counter()\n",
    "\n",
    "# Generate a response\n",
    "input_ids = model.tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "output = model.generate(input_ids, max_new_tokens=1)\n",
    "response = model.tokenizer.decode(output[0][-1:])\n",
    "\n",
    "# End timing for generation\n",
    "end_gen_time = time.perf_counter()\n",
    "generation_time = end_gen_time - start_gen_time\n",
    "\n",
    "# Define function to classify weekday\n",
    "def classify_weekday(text):\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer_roberta(text, return_tensors='pt')\n",
    "    \n",
    "    # Run model\n",
    "    with torch.no_grad():\n",
    "        outputs = classifier(**inputs)\n",
    "    \n",
    "    # For demonstration, we simulate classification since the model isn't fine-tuned for weekdays\n",
    "    weekdays = ['mon', 'monday', 'tue', 'tues', 'tuesday', 'wed', 'wednesday', \n",
    "               'thu', 'thurs', 'thursday', 'fri', 'friday']\n",
    "    \n",
    "    if any(day in text.lower() for day in weekdays):\n",
    "        simulated_logits = torch.tensor([[0.1, 10.0]])  # High confidence for weekday\n",
    "    else:\n",
    "        simulated_logits = torch.tensor([[10.0, 0.1]])  # High confidence for not weekday\n",
    "    \n",
    "    probs = torch.softmax(simulated_logits, dim=1)\n",
    "    predicted_label = torch.argmax(probs, dim=1).item()\n",
    "    \n",
    "    return predicted_label == 1, probs[0, predicted_label].item()\n",
    "\n",
    "# Start timing for classification\n",
    "start_class_time = time.perf_counter()\n",
    "\n",
    "# Classify the response\n",
    "is_weekday, confidence = classify_weekday(response)\n",
    "\n",
    "# End timing for classification\n",
    "end_class_time = time.perf_counter()\n",
    "classification_time = end_class_time - start_class_time\n",
    "\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Model response: {response}\")\n",
    "print(f\"Is weekday: {is_weekday}\")\n",
    "print(f\"Confidence: {confidence:.4f}\")\n",
    "print(f\"Generation time (seconds): {generation_time:.6f}\")\n",
    "print(f\"Classification time (seconds): {classification_time:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "23f86634-ca9f-42ce-971c-f2a92e3b2bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment configuration saved to experiment_configuration.json\n",
      "\n",
      "Summary:\n",
      "System: Linux-5.14.0-284.86.1.el9_2.x86_64-x86_64-with-glibc2.35\n",
      "Python version: 3.12.10\n",
      "Key packages: numpy=1.26.4, pandas=2.2.3, matplotlib=3.10.3...\n",
      "RAM: 377.07 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/state/partition1/job-60609498/ipykernel_2433177/1081351148.py:8: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  import pkg_resources\n"
     ]
    }
   ],
   "source": [
    "# Complete cell to gather computational environment details for research paper\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import platform\n",
    "import datetime\n",
    "from IPython import get_ipython\n",
    "import pkg_resources\n",
    "\n",
    "# System information\n",
    "system_info = {\n",
    "    \"python_version\": sys.version,\n",
    "    \"platform\": platform.platform(),\n",
    "    \"processor\": platform.processor(),\n",
    "    \"timestamp\": datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "}\n",
    "\n",
    "# Get installed packages (focusing on common data science libraries)\n",
    "packages = {}\n",
    "for pkg in pkg_resources.working_set:\n",
    "    packages[pkg.key] = pkg.version\n",
    "\n",
    "key_packages = {}\n",
    "for pkg in ['numpy', 'pandas', 'matplotlib', 'tensorflow', 'torch', 'scikit-learn', \n",
    "            'jupyter', 'ipykernel', 'nbformat']:\n",
    "    if pkg in packages:\n",
    "        key_packages[pkg] = packages[pkg]\n",
    "\n",
    "# Try to get Jupyter config information\n",
    "jupyter_info = {}\n",
    "try:\n",
    "    from jupyter_core.paths import jupyter_config_dir\n",
    "    jupyter_info[\"config_dir\"] = jupyter_config_dir()\n",
    "except:\n",
    "    jupyter_info[\"config_dir\"] = \"Unable to retrieve\"\n",
    "\n",
    "# Try to get hardware utilization\n",
    "hardware_info = {}\n",
    "try:\n",
    "    import psutil\n",
    "    hardware_info = {\n",
    "        \"cpu_count\": psutil.cpu_count(),\n",
    "        \"cpu_percent\": psutil.cpu_percent(),\n",
    "        \"memory_total_gb\": round(psutil.virtual_memory().total / (1024**3), 2),\n",
    "        \"memory_used_percent\": psutil.virtual_memory().percent\n",
    "    }\n",
    "except:\n",
    "    hardware_info = {\"status\": \"psutil not available\"}\n",
    "\n",
    "# Try to get GPU information\n",
    "gpu_info = {}\n",
    "try:\n",
    "    import GPUtil\n",
    "    gpus = GPUtil.getGPUs()\n",
    "    if gpus:\n",
    "        gpu_info = [{\n",
    "            \"name\": gpu.name,\n",
    "            \"memory_total_mb\": gpu.memoryTotal,\n",
    "            \"utilization_percent\": round(gpu.load*100, 1),\n",
    "            \"memory_used_percent\": round(gpu.memoryUsed/gpu.memoryTotal*100, 1)\n",
    "        } for gpu in gpus]\n",
    "    else:\n",
    "        gpu_info = {\"status\": \"No GPUs detected\"}\n",
    "except ImportError:\n",
    "    gpu_info = {\"status\": \"GPUtil not installed\"}\n",
    "\n",
    "# Combine all information\n",
    "experiment_config = {\n",
    "    \"system_info\": system_info,\n",
    "    \"key_packages\": key_packages,\n",
    "    \"jupyter_info\": jupyter_info,\n",
    "    \"hardware_info\": hardware_info,\n",
    "    \"gpu_info\": gpu_info\n",
    "}\n",
    "\n",
    "# Save to file for inclusion in research paper\n",
    "config_file = 'experiment_configuration.json'\n",
    "with open(config_file, 'w') as f:\n",
    "    json.dump(experiment_config, f, indent=2)\n",
    "\n",
    "# Display summary\n",
    "print(f\"Experiment configuration saved to {config_file}\")\n",
    "print(\"\\nSummary:\")\n",
    "print(f\"System: {system_info['platform']}\")\n",
    "print(f\"Python version: {system_info['python_version'].split()[0]}\")\n",
    "print(f\"Key packages: {', '.join([f'{k}={v}' for k,v in key_packages.items()][:3])}...\")\n",
    "if \"memory_total_gb\" in hardware_info:\n",
    "    print(f\"RAM: {hardware_info['memory_total_gb']} GB\")\n",
    "if isinstance(gpu_info, list) and len(gpu_info) > 0:\n",
    "    print(f\"GPU: {gpu_info[0].get('name', 'Unknown')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "19b33829-a546-4a1b-bbca-297eec9692a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d90e16c-d96f-4e07-8d38-1f1f49b05eb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jup_env_0513",
   "language": "python",
   "name": "jup_env_0513"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
